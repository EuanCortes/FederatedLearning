{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the transformation of the data. \n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),     # convert the image to a pytorch tensor\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) # normalise the image with mean and std of 0.5\n",
    "\n",
    "batch_size = 64     # define the batch size\n",
    "\n",
    "trainset = CIFAR10(root='./data', train=True,\n",
    "                   download=True, transform=transform)\n",
    "\n",
    "# split the training set into 80% training and 20% validation\n",
    "train_size = int(0.8 * len(trainset))\n",
    "val_size = len(trainset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(trainset, [train_size, val_size]) #generate random training and validation sets\n",
    "\n",
    "valloader = DataLoader(val_dataset, batch_size=batch_size,\n",
    "                        shuffle=False, num_workers=0)\n",
    "\n",
    "# load test data\n",
    "testset = CIFAR10(root='./data', train=False,\n",
    "                  download=True, transform=transform)\n",
    "testloader = DataLoader(testset, batch_size=batch_size,\n",
    "                        shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "# define classes\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallCNN, self).__init__()\n",
    "\n",
    "        self.relu = nn.ReLU()               # activation function\n",
    "\n",
    "        # convcolutional layers\n",
    "        self.ConvLayers = nn.ModuleList([\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),   # N x 3 x 32 x 32 -> N x 32 x 32 x 32\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),  # N x 32 x 16 x 16 -> N x 64 x 16 x 16\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1), # N x 64 x 8 x 8 -> N x 128 x 8 x 8\n",
    "        ])\n",
    "\n",
    "        # batch normalization layers (for regularization)\n",
    "        self.BatchNorms = nn.ModuleList([\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.BatchNorm2d(128),\n",
    "        ])\n",
    "\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 256)      # first fully connected layer after convolutional layers\n",
    "        self.fc2 = nn.Linear(256, 10)               # final fully connected layer for output\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)      # max pooling layer for regularization\n",
    "        self.dropout = nn.Dropout(0.2)      # dropout layer for regularization\n",
    "\n",
    "    def forward(self, x):\n",
    "        for conv, batchnorm in zip(self.ConvLayers, self.BatchNorms):\n",
    "            # apply convolutional layer, then batch normalization, then ReLU, then max pooling\n",
    "            x = batchnorm(conv(x))\n",
    "            x = self.pool(self.relu(x))       # final shape: N x 128 x 4 x 4\n",
    "\n",
    "        x = x.view(x.size(0), -1)       # reshape to N x 128*4*4\n",
    "        x = self.relu(self.fc1(x))      # fully connected layer and ReLU\n",
    "        x = self.dropout(x)             # apply dropout for regularization\n",
    "        x = self.fc2(x)                 # final fully connected layer for output\n",
    "        return x\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device: {device}\")\n",
    "net = SmallCNN().to(device)\n",
    "\n",
    "trainable_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(\"Trainable parameters:\", trainable_params)\n",
    "\n",
    "init_weights = copy.deepcopy(net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client:\n",
    "    '''\n",
    "    Class that represents clients in a federated learning system.\n",
    "    The client should maintain a fixed local dataset, and\n",
    "    contain a method that can be called to train the model locally\n",
    "    when participating in a round of federated learning.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, net, data, batch_size, criterion, device=None, LR=0.001, weight_decay=0):\n",
    "        '''\n",
    "        Constructor for the Client class.\n",
    "        :param net:         the neural network model\n",
    "        :param data:        the local dataset\n",
    "        :param batch_size:  the batch size to use for training\n",
    "        :param optimizer:   the optimizer to use for training\n",
    "        :param criterion:   the loss function to use for training\n",
    "        :param device:      the device to run the training on (cpu or cuda)\n",
    "        '''\n",
    "        self.net = net.to(device)\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.criterion = criterion\n",
    "\n",
    "        # set device to run the training on\n",
    "        if device is None:\n",
    "            self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            self.device = device\n",
    "\n",
    "        # create a DataLoader for the local dataset\n",
    "        self.dataloader = DataLoader(data, batch_size=batch_size,\n",
    "                                    shuffle=True, num_workers=0)\n",
    "        \n",
    "        # set optimizer\n",
    "        self.optimizer = optim.Adam(net.parameters(), lr=LR, weight_decay=weight_decay)\n",
    "\n",
    "    def train(self, epochs, state_dict):\n",
    "        '''\n",
    "        Training method that trains the model on the local dataset for a number of epochs.\n",
    "        :param epochs:      the number of epochs to train\n",
    "        :param state_dict:  the state dictionary of the global model\n",
    "\n",
    "        :return:            the state dictionary of the trained model and the loss\n",
    "        '''\n",
    "\n",
    "        self.net.load_state_dict(state_dict)    # load global weights\n",
    "        self.net.train()                             # set model to train mode\n",
    "\n",
    "        for epoch in range(epochs):     # iterate thru local epochs\n",
    "            \n",
    "            epoch_loss = 0\n",
    "            for Xtrain, Ytrain in self.dataloader:     # iterate thru local data\n",
    "                Xtrain, Ytrain = Xtrain.to(self.device), Ytrain.to(self.device)\n",
    "\n",
    "                outputs = self.net(Xtrain)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.criterion(outputs, Ytrain)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            \n",
    "                epoch_loss += loss.item() \n",
    "\n",
    "        return copy.deepcopy(self.net.state_dict()), epoch_loss / len(self.dataloader)\n",
    "\n",
    "\n",
    "\n",
    "def FedAvg(state_dicts, weights=None):\n",
    "    '''\n",
    "    Function that averages the weights of the models in the input list.\n",
    "    :param state_dicts:     list of state dictionaries of the models to average\n",
    "    :param weights:         list of weights to use for the averaging\n",
    "\n",
    "    :return:                the state dictionary of the averaged model\n",
    "    '''\n",
    "    avg_state = copy.deepcopy(state_dicts[0])   # copy the first model's weights \n",
    "\n",
    "    for key in avg_state.keys():    # iterate thru the module weights\n",
    "\n",
    "        if weights is not None:     # if weights are provided, use them for the averaging\n",
    "            avg_state[key] = state_dicts[0][key] * weights[0]\n",
    "\n",
    "        for i in range(1, len(state_dicts)):\n",
    "            \n",
    "            if weights is not None:     # if weights are provided, use them for the averaging\n",
    "                avg_state[key] += state_dicts[i][key] * weights[i]\n",
    "\n",
    "            else:\n",
    "                avg_state[key] += state_dicts[i][key]\n",
    "\n",
    "        avg_state[key] = avg_state[key] / len(state_dicts)\n",
    "\n",
    "    return avg_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples per client: 400\n",
      "Clients per round: 10\n"
     ]
    }
   ],
   "source": [
    "NUM_CLIENTS = 100        # number of clients\n",
    "IID_DATA = True          # whether the data is IID or not\n",
    "NUM_ROUNDS = 50          # number of rounds of federated learning\n",
    "NUM_LOCAL_EPOCHS = 20    # number of local epochs\n",
    "C = 0.1                  # fraction of clients to use per round\n",
    "CLIENTS_PER_ROUND = int(NUM_CLIENTS * C)    # number of clients to use per round\n",
    "LR = 0.001                      # learning rate\n",
    "DATA_SIZE = len(train_dataset)  # size of the training dataset\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#create indices for each client\n",
    "if IID_DATA:\n",
    "    client_indices = torch.tensor_split(torch.randperm(DATA_SIZE), NUM_CLIENTS)\n",
    "else:\n",
    "    raise NotImplementedError(\"Non-IID data not implemented\")\n",
    "\n",
    "clients = [\n",
    "    Client(net=SmallCNN(), \n",
    "           data=Subset(train_dataset, indices), \n",
    "           batch_size=batch_size, \n",
    "           criterion=criterion, \n",
    "           device=device, \n",
    "           LR=LR)\n",
    "    for indices in client_indices\n",
    "]\n",
    "\n",
    "print(f\"Samples per client: {len(client_indices[0]):d}\")\n",
    "print(f\"Clients per round: {CLIENTS_PER_ROUND:d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1 done\n",
      "training loss: 0.096\n",
      "Federated Averaging done\n",
      "Round 2 done\n",
      "training loss: 0.048\n",
      "Federated Averaging done\n",
      "Round 3 done\n",
      "training loss: 0.026\n",
      "Federated Averaging done\n",
      "Round 4 done\n",
      "training loss: 0.018\n",
      "Federated Averaging done\n",
      "Round 5 done\n",
      "training loss: 0.024\n",
      "Federated Averaging done\n",
      "Test accuracy in round 5: 64 %\n",
      "Round 6 done\n",
      "training loss: 0.016\n",
      "Federated Averaging done\n",
      "Round 7 done\n",
      "training loss: 0.012\n",
      "Federated Averaging done\n",
      "Round 8 done\n",
      "training loss: 0.018\n",
      "Federated Averaging done\n",
      "Round 9 done\n",
      "training loss: 0.012\n",
      "Federated Averaging done\n",
      "Round 10 done\n",
      "training loss: 0.020\n",
      "Federated Averaging done\n",
      "Test accuracy in round 10: 65 %\n",
      "Round 11 done\n",
      "training loss: 0.018\n",
      "Federated Averaging done\n",
      "Round 12 done\n",
      "training loss: 0.009\n",
      "Federated Averaging done\n",
      "Round 13 done\n",
      "training loss: 0.015\n",
      "Federated Averaging done\n",
      "Round 14 done\n",
      "training loss: 0.011\n",
      "Federated Averaging done\n",
      "Round 15 done\n",
      "training loss: 0.009\n",
      "Federated Averaging done\n",
      "Test accuracy in round 15: 68 %\n",
      "Round 16 done\n",
      "training loss: 0.016\n",
      "Federated Averaging done\n",
      "Round 17 done\n",
      "training loss: 0.016\n",
      "Federated Averaging done\n",
      "Round 18 done\n",
      "training loss: 0.014\n",
      "Federated Averaging done\n",
      "Round 19 done\n",
      "training loss: 0.015\n",
      "Federated Averaging done\n",
      "Round 20 done\n",
      "training loss: 0.014\n",
      "Federated Averaging done\n",
      "Test accuracy in round 20: 70 %\n",
      "Round 21 done\n",
      "training loss: 0.006\n",
      "Federated Averaging done\n",
      "Round 22 done\n",
      "training loss: 0.011\n",
      "Federated Averaging done\n",
      "Round 23 done\n",
      "training loss: 0.008\n",
      "Federated Averaging done\n",
      "Round 24 done\n",
      "training loss: 0.007\n",
      "Federated Averaging done\n",
      "Round 25 done\n",
      "training loss: 0.013\n",
      "Federated Averaging done\n",
      "Test accuracy in round 25: 72 %\n",
      "Round 26 done\n",
      "training loss: 0.010\n",
      "Federated Averaging done\n",
      "Round 27 done\n",
      "training loss: 0.008\n",
      "Federated Averaging done\n",
      "Round 28 done\n",
      "training loss: 0.013\n",
      "Federated Averaging done\n",
      "Round 29 done\n",
      "training loss: 0.007\n",
      "Federated Averaging done\n",
      "Round 30 done\n",
      "training loss: 0.008\n",
      "Federated Averaging done\n",
      "Test accuracy in round 30: 72 %\n",
      "Round 31 done\n",
      "training loss: 0.009\n",
      "Federated Averaging done\n",
      "Round 32 done\n",
      "training loss: 0.015\n",
      "Federated Averaging done\n",
      "Round 33 done\n",
      "training loss: 0.007\n",
      "Federated Averaging done\n",
      "Round 34 done\n",
      "training loss: 0.007\n",
      "Federated Averaging done\n",
      "Round 35 done\n",
      "training loss: 0.008\n",
      "Federated Averaging done\n",
      "Test accuracy in round 35: 73 %\n",
      "Round 36 done\n",
      "training loss: 0.003\n",
      "Federated Averaging done\n",
      "Round 37 done\n",
      "training loss: 0.014\n",
      "Federated Averaging done\n",
      "Round 38 done\n",
      "training loss: 0.010\n",
      "Federated Averaging done\n",
      "Round 39 done\n",
      "training loss: 0.004\n",
      "Federated Averaging done\n",
      "Round 40 done\n",
      "training loss: 0.005\n",
      "Federated Averaging done\n",
      "Test accuracy in round 40: 73 %\n",
      "Round 41 done\n",
      "training loss: 0.004\n",
      "Federated Averaging done\n",
      "Round 42 done\n",
      "training loss: 0.005\n",
      "Federated Averaging done\n",
      "Round 43 done\n",
      "training loss: 0.010\n",
      "Federated Averaging done\n",
      "Round 44 done\n",
      "training loss: 0.003\n",
      "Federated Averaging done\n",
      "Round 45 done\n",
      "training loss: 0.004\n",
      "Federated Averaging done\n",
      "Test accuracy in round 45: 74 %\n",
      "Round 46 done\n",
      "training loss: 0.005\n",
      "Federated Averaging done\n",
      "Round 47 done\n",
      "training loss: 0.007\n",
      "Federated Averaging done\n",
      "Round 48 done\n",
      "training loss: 0.004\n",
      "Federated Averaging done\n",
      "Round 49 done\n",
      "training loss: 0.003\n",
      "Federated Averaging done\n",
      "Round 50 done\n",
      "training loss: 0.003\n",
      "Federated Averaging done\n",
      "Test accuracy in round 50: 74 %\n"
     ]
    }
   ],
   "source": [
    "avg_test_loss = []\n",
    "\n",
    "current_weights = copy.deepcopy(init_weights)\n",
    "\n",
    "for round in range(NUM_ROUNDS):     # iterate thru rounds\n",
    "\n",
    "    current_weights_cpu = {k: v.cpu() for k, v in current_weights.items()}\n",
    "\n",
    "    client_ids = torch.randperm(NUM_CLIENTS)[:CLIENTS_PER_ROUND] # random selection of clients to participate\n",
    "    local_weights = []\n",
    "    temp_avg_loss = 0\n",
    "    for id in client_ids:   # iterate thru clients\n",
    "\n",
    "        state_dict, loss = clients[id].train(NUM_LOCAL_EPOCHS, current_weights)\n",
    "        local_weights.append(state_dict)\n",
    "        temp_avg_loss += loss\n",
    "\n",
    "\n",
    "    avg_test_loss.append(temp_avg_loss / CLIENTS_PER_ROUND)\n",
    "    \n",
    "    print(f\"Round {round+1} done\")\n",
    "    print(f\"training loss: {avg_test_loss[-1]:.3f}\")\n",
    "    # average local weights\n",
    "    #new_weights = {}\n",
    "    #for key in current_weights:\n",
    "    #    new_weights[key] = torch.stack([local_weights[i][key] for i in range(CLIENTS_PER_ROUND)]).sum(0) / CLIENTS_PER_ROUND\n",
    "    new_weights = FedAvg(local_weights)    \n",
    "    print(\"Federated Averaging done\")\n",
    "\n",
    "    current_weights = new_weights\n",
    "\n",
    "    if round % 5 == 4:\n",
    "        net.load_state_dict(current_weights)\n",
    "        net.eval()\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for (images, labels) in valloader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                # calculate outputs by running images through the network\n",
    "                outputs = net(images)\n",
    "                # the class with the highest energy is what we choose as prediction\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f'Test accuracy in round {round+1:d}: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 74 %\n",
      "Accuracy for class: plane is 78.2 %\n",
      "Accuracy for class: car   is 87.9 %\n",
      "Accuracy for class: bird  is 59.2 %\n",
      "Accuracy for class: cat   is 51.7 %\n",
      "Accuracy for class: deer  is 71.3 %\n",
      "Accuracy for class: dog   is 59.2 %\n",
      "Accuracy for class: frog  is 88.3 %\n",
      "Accuracy for class: horse is 81.5 %\n",
      "Accuracy for class: ship  is 86.5 %\n",
      "Accuracy for class: truck is 81.4 %\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for (images, labels) in testloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
    "\n",
    "# prepare to count predictions for each class\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for (images, labels) in testloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FLenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
